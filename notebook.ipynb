{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSC 80 Hypothesis Testing Review\n",
    "\n",
    "## Coin Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "pd.options.plotting.backend = 'plotly'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Null Hypothesis: The coin is fair. It lands on heads and tails equally. `P(H) = P(T) = 0.5`\n",
    "\n",
    "Alternative Hypothesis: The coin is not fair. The probability of landing on heads is not 0.5. `P(H) != 0.5`\n",
    "- Note that we did not specify a direction, so this is a **two-sided** test.\n",
    "\n",
    "Test Statistic: Total variance distance (TVD)\n",
    "- Coin flips are categorical!\n",
    "- Note that we could also use the number of heads/tails, like we did in the pre-lecture\n",
    "\n",
    "Significance Level: 0.05 or your choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total Variation Distance formula - our test statistic\n",
    "\n",
    "# Calculate TVDs for many rows of distributions\n",
    "def tvds(dist1, dist2):\n",
    "    return np.sum(np.abs(dist1 - dist2), axis=1) / 2\n",
    "\n",
    "# Calculate TVD for one distribution\n",
    "def tvd(dist1, dist2):\n",
    "    return np.sum(np.abs(dist1 - dist2)) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`simulation_coinflips` takes in 3 arguments:\n",
    "- `data`: The count of [heads, tails] in each set of flips, as a 2D array.\n",
    "- `observed`: The observed count of [heads, tails], as a 1D array.\n",
    "- `alpha`: The chosen significance level, as a decimal. Default is 0.05.\n",
    "\n",
    "Output a histogram showing the empirical distribution of the test statistic and highlight the observed statistic with a red line. Print the p value along with whether you reject or fail to reject the null.\n",
    "\n",
    "Check out these: \n",
    "- <a href='https://www.mathmammoth.com/practice/coin-tosser'>Virtual coin flipper</a> - to create your own data\n",
    "- `px.histogram` <a href='https://plotly.github.io/plotly.py-docs/generated/plotly.express.histogram.html'>here</a> (Hint: use `histnorm='probability'`)\n",
    "- `.add_vline` <a href='https://plotly.com/python/horizontal-vertical-shapes/#horizontal-and-vertical-lines-and-rectangles'>here</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input your data here! In the form [# of heads, # of tails]\n",
    "\n",
    "# data = [[],[],[],[],[]]\n",
    "# observed = []\n",
    "# alpha = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_coinflips(data, observed, alpha=0.05):\n",
    "    # 1. Given the [heads, tails] counts, find the distribution of [heads, tails] in each sample.\n",
    "    #    Do this for both the data and observed counts.\n",
    "    data = ...\n",
    "    observed = ...\n",
    "\n",
    "    # 2. Find the TVD of each pair of [heads, tails] and the expected proportion [0.5, 0.5]\n",
    "    #    Do this for both the data and observed distributions. Feel free to use the tvds() and tvd() functions above.\n",
    "    sample_tvds = ...\n",
    "    observed_tvd = ...\n",
    "\n",
    "    # 3. Compute the p-value. Should we calculate the proportion of TVDs greater than or less than our observed?\n",
    "    p = ...\n",
    "\n",
    "    # 4. Visualize. Don't forget a title!\n",
    "    fig = px.histogram(...)\n",
    "    fig.add_vline(...)\n",
    "    fig.add_annotation(text=f'{round(observed_tvd, 2)}', x=observed_tvd, y=1.06, yref='paper', showarrow=False)\n",
    "    fig.update_layout(xaxis_title='Total Variation Distance',\n",
    "                      yaxis_title='Proportion')\n",
    "    \n",
    "    # 5. Reject or fail to reject?\n",
    "    print(f'The p-value is {p} and the observed tvd is {round(observed_tvd, 2)}.')\n",
    "    if p < alpha:\n",
    "        print('Reject the null hypothesis. The coin is not fair!')\n",
    "    else:\n",
    "        print('Failed to reject the null.')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function using the variables you inputted above.\n",
    "\n",
    "# simulation_coinflips(data, observed, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out the documentation for `np.random.multinomial` <a href='https://numpy.org/doc/2.0/reference/random/generated/numpy.random.multinomial.html'>here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[244, 256],\n",
       "       [261, 239],\n",
       "       [241, 259],\n",
       "       [257, 243],\n",
       "       [244, 256]])"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Outputs the count of [heads, tails] in each set of 500 coin flips.\n",
    "# The size argument allows us to repeat the simulation without a for-loop!\n",
    "\n",
    "np.random.multinomial(500, [0.5, 0.5], size=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't want to make your own data, try out this general function \n",
    "# that uses np.random.multinomial and alpha = 0.05 instead.\n",
    "\n",
    "def random_coinflips():\n",
    "    # Our sample contains 500 fair coins. Flip all 500 coins, 1000 times. \n",
    "    data = np.random.multinomial(500, [0.5, 0.5], size=1000)\n",
    "\n",
    "    # Get the proportion of [heads, tails] in each set of 500 coin flips.\n",
    "    data = data / 500\n",
    "\n",
    "    # Our observed data contains 100 unfair coins. Flip all 100 coins, once.\n",
    "    # Get the proportion of [heads, tails] in each set of 100 coin flips.\n",
    "    observed = np.random.multinomial(100, [0.7, 0.3]) / 100\n",
    "\n",
    "    # Find the TVD of each pair of [heads, tails] and the expected proportion [0.5, 0.5] \n",
    "    sample_tvds = tvds(data, [0.5, 0.5])\n",
    "    observed_tvd = tvd(observed, [0.5, 0.5])\n",
    "\n",
    "    # Compute the p-value. Larger TVDs reflect greater unfairness, \n",
    "    # so we calculate the proportion of distances >= our observed TVD.\n",
    "    p = np.mean(sample_tvds >= observed_tvd)\n",
    "\n",
    "    # Visualize\n",
    "    fig = px.histogram(sample_tvds, histnorm='probability', \n",
    "                       title='Empirical Distribution of the TVD of Fair Coin Flips')\n",
    "    fig.add_vline(x=observed_tvd, line_color='red', line_width=2, opacity=1)\n",
    "    fig.add_annotation(text=f'{round(observed_tvd, 2)}', x=observed_tvd, y=1.06, yref='paper', showarrow=False)\n",
    "    fig.update_layout(xaxis_title='Total Variation Distance',\n",
    "                      yaxis_title='Proportion',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    # Reject or fail to reject?\n",
    "    print(f'The p-value is {p} and the observed tvd is {round(observed_tvd, 2)}.')\n",
    "    if p < 0.05:\n",
    "        print('Reject the null hypothesis. The coin is not fair!')\n",
    "    else:\n",
    "        print('Failed to reject the null.')\n",
    "        \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random_coinflips()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Guided Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we conducted a two-sided test for coin flips, try making a one-sided test under the following hypotheses:\n",
    "\n",
    "- Null Hypothesis: The coin is fair. It lands on heads and tails equally. `P(H) = P(T) = 0.5` (same as before)\n",
    "- Alternative Hypothesis: The coin is not fair. The probability of landing on heads is **greater than** 0.5. (indicates a direction)\n",
    "\n",
    "With these hypotheses, we suspect that the coin favors heads. That is, instead of saying, `P(H) != 0.5`, we are saying `P(H) > 0.5`.\n",
    "\n",
    "Since this is a one-sided test, we should **not** use the total variation distance as our test statistic. Instead, we might choose:\n",
    "- number of heads (like in the pre-lecture)\n",
    "- proportion of heads\n",
    "- number of tails\n",
    "- proportion of tails\n",
    "\n",
    "You may choose any of these (or come up with your own) in your implementation.\n",
    "\n",
    "`one-sided` takes in the 2 arguments:\n",
    "- `observed`: The observed count of [heads, tails], as a 1D array.\n",
    "- `alpha`: The chosen significance level, as a decimal. Default is 0.05.\n",
    "\n",
    "Use `np.random.multinomial` to create the sample data. Feel free to use the virtual coin flipper for your observed data.\n",
    "\n",
    "Output a histogram showing the empirical distribution of the test statistic and highlight the observed statistic with a red line. Print the p value along with whether you reject or fail to reject the null. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_sided(observed, alpha=0.05):\n",
    "    # 1. Generate sample counts/proportions of [heads, tails] for 100 fair coins. Flip all 100 coins, 50 times.\n",
    "    #    Find observed proportions if necessary.\n",
    "    data = ...\n",
    "    observed = ...\n",
    "\n",
    "    # 2. Calculate the 50 test statistics and your observed test statistic.\n",
    "    sample_tests = ...\n",
    "    observed_test = ...\n",
    "\n",
    "    # 3. Calculate the p-value. Since we want to find if the coin favors heads, \n",
    "    #    should we find the proportion of sample tests >= or <= the observed?\n",
    "    #    Varies for different test statistics.\n",
    "    p = ...\n",
    "\n",
    "    # 4. Visualize. Fill in the blanks depending on your test statistic!\n",
    "    fig = px.histogram(sample_tests, histnorm='probability', \n",
    "                       title='Empirical Distribution of ____')\n",
    "    fig.add_vline(x=observed_test, line_color='red', line_width=2, opacity=1)\n",
    "    fig.add_annotation(text=f'{round(observed_test, 2)}', x=observed_test, y=1.06, yref='paper', showarrow=False)\n",
    "    fig.update_layout(xaxis_title='________',\n",
    "                      yaxis_title='Proportion',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    # 5. Reject or fail to reject?\n",
    "    print(f'The p-value is {p} and the observed test statistic is {round(observed_test, 2)}.')\n",
    "    if p < alpha:\n",
    "        print('Reject the null hypothesis. The coin favors heads!')\n",
    "    else:\n",
    "        print('Failed to reject the null.')\n",
    "        \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try out your function here!\n",
    "\n",
    "# observed = []\n",
    "# alpha = ...\n",
    "\n",
    "# one_sided(observed, alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulation_coinflips_ans(data, observed, alpha=0.05):\n",
    "    # Get the proportion of [heads, tails] in each set of fair coin flips.\n",
    "    data = data / np.sum(data, axis=1, keepdims=True)\n",
    "    observed = observed / np.sum(observed)\n",
    "\n",
    "    # Find the TVD of each pair of [heads, tails] and the expected proportion [0.5, 0.5]\n",
    "    sample_tvds = tvds(data, [0.5, 0.5])\n",
    "    observed_tvd = tvd(observed, [0.5, 0.5])\n",
    "\n",
    "    # Compute the p-value. Larger TVDs reflect greater unfairness, \n",
    "    # so we calculate the proportion of distances >= our observed TVD.\n",
    "    p = np.mean(sample_tvds >= observed_tvd)\n",
    "\n",
    "    # Visualize\n",
    "    fig = px.histogram(sample_tvds, histnorm='probability', \n",
    "                       title='Empirical Distribution of the TVD of Fair Coin Flips')\n",
    "    fig.add_vline(x=observed_tvd, line_color='red', line_width=2, opacity=1)\n",
    "    fig.add_annotation(text=f'{round(observed_tvd, 2)}', x=observed_tvd, y=1.06, yref='paper', showarrow=False)\n",
    "    fig.update_layout(xaxis_title='Total Variation Distance',\n",
    "                      yaxis_title='Proportion',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    # Reject or fail to reject?\n",
    "    print(f'The p-value is {p} and the observed tvd is {round(observed_tvd, 2)}')\n",
    "    if p < alpha:\n",
    "        print('Reject the null hypothesis. The coin is not fair!')\n",
    "    else:\n",
    "        print('Failed to reject the null.')\n",
    "    \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This solution uses the proportion of tails as the test statistic.\n",
    "\n",
    "def one_sided_ans(observed, alpha=0.05):\n",
    "    # 1. Generate sample counts/proportions of [heads, tails] for 100 fair coins. Flip all 100 coins, 50 times.\n",
    "    #    Find observed proportions if necessary.\n",
    "    data = np.random.multinomial(100, [0.5, 0.5], size=50) / 100\n",
    "    observed = np.array(observed) / np.sum(observed)\n",
    "\n",
    "    # 2. Calculate the 50 test statistics and your observed test statistic.\n",
    "    #    Tails proportions are in the 2nd column.\n",
    "    sample_tests = data[:, 1]\n",
    "    observed_test = observed[1]\n",
    "\n",
    "    # 3. Calculate the p-value. Since we want to find if the coin favors heads, \n",
    "    #    should we find the proportion of sample tests >= or <= the observed?\n",
    "    #    Varies for different test statistics.\n",
    "\n",
    "    # If our observed coin favor heads, we should find the proportion of \n",
    "    # sample tails proportions less than the observed.\n",
    "    p = np.mean(sample_tests <= observed_test)\n",
    "\n",
    "    # 4. Visualize. Fill in the blanks depending on your test statistic!\n",
    "    fig = px.histogram(sample_tests, histnorm='probability', \n",
    "                       title='Empirical Distribution of the Proportion of Tails')\n",
    "    fig.add_vline(x=observed_test, line_color='red', line_width=2, opacity=1)\n",
    "    fig.add_annotation(text=f'{round(observed_test, 2)}', x=observed_test, y=1.06, yref='paper', showarrow=False)\n",
    "    fig.update_layout(xaxis_title='Proportion of Tails',\n",
    "                      yaxis_title='Proportion',\n",
    "                      showlegend=False)\n",
    "    \n",
    "    # 5. Reject or fail to reject?\n",
    "    print(f'The p-value is {p} and the observed test statistic is {round(observed_test, 2)}.')\n",
    "    if p < alpha:\n",
    "        print('Reject the null hypothesis. The coin favors heads!')\n",
    "    else:\n",
    "        print('Failed to reject the null.')\n",
    "        \n",
    "    return fig"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
